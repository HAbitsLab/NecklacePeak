# wild_segmentation_unified.py is using an identical parameter setting, maybe generate worse result.
# NOTE: FOR 110, 114, 116, 118, 121, peaksIndex = peak_detection(proximity, min_prominence=3)

import os
import sys
import logging
import numpy as np
import pandas as pd
from datetime import date, time, datetime
sys.path.append('..')
from beyourself.core.algorithm import interval_intersect_interval
sys.path.append('../mining')
from periodic import periodic_subsequence, get_periodic_stat, peak_detection
from utils import create_folder, list_files_in_directory, df_to_datetime_tz_aware, datetime_str_to_unixtime
from settings import settings
sys.path.append('../data_collect/necklace')
from read_data import read_data

logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)


# ==================================================================================
ROOT_DIR = settings['ROOT_DIR']
# subj = settings['subj']
subj = str(sys.argv[1])
# subj = '208'
startDate = settings['START_DATE']
calendarDayAmendHours = settings['CALENDAR_DAY_AMEND_HOURS']

ANNO_FOLDER = os.path.join(ROOT_DIR, 'ANNOTATION', subj, 'CHEWING')
DATA_FOLDER = os.path.join(ROOT_DIR, 'CLEAN', subj, 'NECKLACE')
TEMP_FOLDER = os.path.join(ROOT_DIR, 'pallas/wild', subj)
# ==================================================================================
create_folder(TEMP_FOLDER)

segFile = 'segments_' + subj + '.csv'
# segFile = 'segments_' + subj + '_unsampled.csv'

#inclusion = pd.read_csv(os.path.join(ANNO_FOLDER, 'inclusion.csv'))
dfChewing = pd.read_csv(os.path.join(ANNO_FOLDER, 'chewing.csv'))
dfChewing = df_to_datetime_tz_aware(dfChewing, ['start', 'end'])

# intervalChewing = list(zip(dfChewing['start'].tolist(), dfChewing['end'].tolist()))
# # checking that dfChewing is monotonic, no overlapping between chewing segments
# # avoid human error in labeling
# for i in range(1, dfChewing.shape[0]):
#     diff = dfChewing['start'].iloc[i] - dfChewing['end'].iloc[i - 1]

#     if not diff >= pd.Timedelta(0):
#         bad = "Double check data!!! {}||{}||{}||{}".format(i, diff, dfChewing['start'].iloc[i],
#                                                             dfChewing['end'].iloc[i - 1])
#         raise ValueError(bad)

segmentationConcatList = []

print('Hello.')

files = list_files_in_directory(DATA_FOLDER)

for file in files:

   	dfSensor = pd.read_csv(os.path.join(DATA_FOLDER,file))
   	print(dfSensor)
    timeArr = dfSensor['Time'].values
    proxArr = dfSensor['proximity'].values
    # find_nonmonotonic(timeArr)
    # be careful to pick prominence (check data is normalized or not)
    peaksIndex = peak_detection(proxArr, min_prominence=2)
    peaksTime = timeArr[peaksIndex]
    subsequences = periodic_subsequence(peaksIndex, peaksTime, min_length=4, max_length=100,
                                        eps=0.1, alpha=0.45, low=400, high=1200)
    print('# subsequences: ', str(len(subsequences)))
    if len(subsequences) == 0:
        continue

    segments = []
    for index in subsequences:
        seq = timeArr[index]
        segments.append(get_periodic_stat(seq))

    dfSubSegment = pd.DataFrame(segments, columns=['start', 'end', 'eps', 'pmin', 'pmax', 'length'])
    segmentationConcatList.append(dfSubSegment)

assert len(segmentationConcatList) != 0, "First check if you have the data files in directory " + DATA_FOLDER
dfSegments = pd.concat(segmentationConcatList)
dfSegments = dfSegments.drop_duplicates().reset_index(drop=True)

intervalSegments = list(zip(dfSegments['start'].tolist(), dfSegments['end'].tolist()))

# calculating ground truth
intersect = interval_intersect_interval(groundtruth=intervalChewing,
                                        prediction=intervalSegments)
print("Recall: {}".format(intersect['recall']))
dfSegments['chewing_gt'] = intersect['prediction_gt']

# calculate date of experiment
dateExp = []
for i in range(dfSegments.shape[0]):
    # dfSegments['start'][i] is generated by epoch_to_datetime, with timezone info
    dateExp.append((dfSegments['start'][i] - \
        datetime.combine(startDate[subj], time(hour=calendarDayAmendHours)).astimezone(settings["TIMEZONE"])).days)

dfSegments['date_exp'] = pd.Series(dateExp).values
# dfSegments = dfSegments.drop_duplicates(keep=False).reset_index(drop=True)
dfSegments.to_csv(os.path.join(TEMP_FOLDER, segFile), index=None)

